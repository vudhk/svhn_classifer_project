{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as io\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np\n",
    "import time\n",
    "import traceback\n",
    "from tensorflow.python.ops import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dinh nghia cac tham so mac dinh\n",
    "VALIDATION_SIZE = 14652\n",
    "TRAINING_INTERS = 30\n",
    "BATCH_SIZE = 200\n",
    "N_INPUT = 32\n",
    "N_CLASSES = 10\n",
    "N_CHANNEL = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    train_data = io.loadmat('../svhn_data/train_32x32.mat', variable_names='X').get('X').transpose([3,0,1,2])\n",
    "    train_label = io.loadmat('../svhn_data/train_32x32.mat', variable_names='y').get('y')\n",
    "    test_data = io.loadmat('../svhn_data/test_32x32.mat', variable_names='X').get('X').transpose([3,0,1,2])\n",
    "    test_label = tf.one_hot(np.squeeze(io.loadmat('../svhn_data/test_32x32.mat', variable_names='y').get('y')), N_CLASSES)\n",
    "    validation_data = train_data[:VALIDATION_SIZE]\n",
    "    validation_label = tf.one_hot(np.squeeze(train_label[:VALIDATION_SIZE]), N_CLASSES)\n",
    "    train_data = train_data[VALIDATION_SIZE:]\n",
    "    train_label = tf.one_hot(np.squeeze(train_label[VALIDATION_SIZE:]), N_CLASSES)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        train_label, validation_label, test_label = sess.run([train_label, validation_label, test_label])\n",
    "\n",
    "    \n",
    "    return {'data': train_data, 'label': train_label},{'data': validation_data, 'label': validation_label},{'data': test_data, 'label': test_label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xay dung mo hinh cnn su dung slim\n",
    "def cnn_1(input, is_training=True):\n",
    "    # su dung arg_scope de chac chan rang moi layer chi su dung cung mot gia tri cua cac parameter\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected], \n",
    "                            normalizer_fn=slim.batch_norm):\n",
    "    \n",
    "        with slim.arg_scope([slim.conv2d], padding='SAME'):\n",
    "            \n",
    "            tf.summary.histogram('input', input)\n",
    "            tf.summary.image('input', input[:,:,:,:3])\n",
    "            \n",
    "            # Convolution layer 1\n",
    "            net = slim.conv2d(input, 32, 5, scope='conv1')\n",
    "            tf.summary.histogram('conv1', net)\n",
    "            \n",
    "            net = slim.max_pool2d(net, 2, scope='pool1')\n",
    "            tf.summary.image('pool1', net[:,:,:,:3])\n",
    "            \n",
    "            # Convolution layer 2\n",
    "            net = slim.conv2d(net, 64, 5, scope='conv2')\n",
    "            tf.summary.histogram('conv2', net)\n",
    "            \n",
    "            net = slim.max_pool2d(net, 2, scope='pool2')\n",
    "            tf.summary.image('pool2', net[:,:,:,:3])\n",
    "            \n",
    "            # Convolution layer 3\n",
    "            net = slim.conv2d(net, 128, 5, scope='conv3')\n",
    "            tf.summary.histogram('conv3', net)\n",
    "            \n",
    "            net = slim.max_pool2d(net, 2, scope='pool3')\n",
    "            tf.summary.image('pool3', net[:,:,:,:3])\n",
    "            \n",
    "            net = slim.flatten(net, scope='flatten1')\n",
    "            \n",
    "        # Fully connected layer 1\n",
    "        net = slim.fully_connected(net, 1024, scope='fc1')\n",
    "        tf.summary.histogram('fc1', net)\n",
    "        net = slim.dropout(net, is_training=is_training, scope='dropout1')  # 0.5 by default\n",
    "        \n",
    "        # Fully connected layer 1\n",
    "        outputs = slim.fully_connected(net, N_CLASSES, activation_fn=None, scope='fco')\n",
    "        tf.summary.histogram('fco', net)\n",
    "        \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Re-shaped AlexNet\n",
    "def cnn_2(input, is_training=True):\n",
    "    # su dung arg_scope de chac chan rang moi layer chi su dung cung mot gia tri cua cac parameter\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected], \n",
    "                            normalizer_fn=slim.batch_norm):\n",
    "    \n",
    "        with slim.arg_scope([slim.conv2d], padding='SAME'):\n",
    "            \n",
    "            tf.summary.histogram('input', input)\n",
    "            tf.summary.image('input', input[:,:,:,:3])\n",
    "            \n",
    "            # Convolution layer 1\n",
    "            net = slim.conv2d(input, 48, 8, stride=3, scope='conv1')\n",
    "            tf.summary.histogram('conv1', net)\n",
    "            \n",
    "            net = slim.max_pool2d(net, 2, stride=1, scope='pool1')\n",
    "            tf.summary.image('pool1', net[:,:,:,:3])\n",
    "            \n",
    "            net = tf.nn.local_response_normalization(net)\n",
    "            \n",
    "            # Convolution layer 2\n",
    "            net = slim.conv2d(net, 126, 2, stride=1, scope='conv2')\n",
    "            tf.summary.histogram('conv2', net)\n",
    "            \n",
    "            net = slim.max_pool2d(net, 2, stride=1, scope='pool2')\n",
    "            tf.summary.image('pool2', net[:,:,:,:3])\n",
    "            \n",
    "            net = tf.nn.local_response_normalization(net)\n",
    "            \n",
    "            # Convolution layer 3\n",
    "            net = slim.conv2d(net, 192, 3, stride=1, scope='conv3')\n",
    "            tf.summary.histogram('conv3', net)\n",
    "            \n",
    "            # Convolution layer 4\n",
    "            net = slim.conv2d(net, 192, 2, stride=1, scope='conv4')\n",
    "            tf.summary.histogram('conv4', net)\n",
    "            \n",
    "            # Convolution layer 5\n",
    "            net = slim.conv2d(net, 128, 2, stride=1, scope='conv5')\n",
    "            tf.summary.histogram('conv5', net)\n",
    "            \n",
    "            net = slim.max_pool2d(net, 2, stride=1, scope='pool3')\n",
    "            tf.summary.image('pool3', net[:,:,:,:3])\n",
    "            \n",
    "            net = tf.nn.local_response_normalization(net)\n",
    "            \n",
    "            net = slim.flatten(net, scope='flatten1')\n",
    "            \n",
    "        # Fully connected layer 1\n",
    "        net = slim.fully_connected(net, 1024, scope='fc1')\n",
    "        tf.summary.histogram('fc1', net)\n",
    "        net = slim.dropout(net, is_training=is_training, scope='dropout1')  # 0.5 by default\n",
    "        \n",
    "         # Fully connected layer 2\n",
    "        net = slim.fully_connected(net, 1024,scope='fc2')\n",
    "        tf.summary.histogram('fc2', net)\n",
    "        net = slim.dropout(net, is_training=is_training, scope='dropout2')\n",
    "        \n",
    "         # Fully connected layer 3\n",
    "        outputs = slim.fully_connected(net, N_CLASSES, activation_fn=nn.softmax, scope='fco')\n",
    "        tf.summary.histogram('fco', net)\n",
    "        \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "        sess.run(init, feed_dict={is_training:True})\n",
    "        # Training cycle\n",
    "        for epoch in range(TRAINING_INTERS):\n",
    "            total_batch =  train_len // BATCH_SIZE\n",
    "            for batch in range(total_batch):\n",
    "                # lay batch tiep theo\n",
    "                batch_input = train_set['data'][batch*BATCH_SIZE:min((batch+1)*BATCH_SIZE,train_len)]\n",
    "                batch_label = train_set['label'][batch*BATCH_SIZE:min((batch+1)*BATCH_SIZE,train_len)]\n",
    "                # chay train_op, loss_op, accuracy\n",
    "                _, cost, acc, summary = sess.run([train_op, loss_op, accuracy, merged_summary_op], feed_dict={input:batch_input, label:batch_label, is_training:True})\n",
    "                # Write logs at every iteration\n",
    "                writer.add_summary(summary, epoch * total_batch + batch)\n",
    "\n",
    "            # hien thi ket qua sau moi epoch\n",
    "            file_writer.write(\"Epoch:\" + ('%04d,' % (epoch + 1)) + (\"cost={%.9f}, training accuracy %.5f\" % (cost, acc)) + \"\\n\")\n",
    "            file_writer.flush()\n",
    "\n",
    "        file_writer.write('Optimization completed!!!\\n')\n",
    "\n",
    "        # Luu tru variables vao disk.\n",
    "        save_path = saver.save(sess, model_path)\n",
    "        file_writer.write(\"Model saved in path: %s \\n\" % save_path)\n",
    "        file_writer.flush()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalute(dataset, model_path):\n",
    "    dataset_len = dataset['data'].shape[0]\n",
    "    # Su dung model da duoc luu de tien doan\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, model_path)\n",
    "        avg_acc = 0.\n",
    "        total_batch =  dataset_len // BATCH_SIZE\n",
    "        for batch in range(total_batch):\n",
    "            # lay batch tiep theo\n",
    "            batch_input = dataset['data'][batch*BATCH_SIZE:min((batch+1)*BATCH_SIZE,dataset_len)]\n",
    "            batch_label = dataset['label'][batch*BATCH_SIZE:min((batch+1)*BATCH_SIZE,dataset_len)]    \n",
    "            acc = sess.run(accuracy, feed_dict={input:batch_input, label:batch_label, is_training:False})\n",
    "            avg_acc += acc / total_batch\n",
    "            \n",
    "        file_writer.write(\"Accuracy on test set: %.5f \\n\" % (avg_acc))\n",
    "        file_writer.flush()\n",
    "        \n",
    "    return avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    file_writer = open('logfile.log', 'w')\n",
    "\n",
    "    # Lay va phan chia data tu file\n",
    "    train_set, validation_set, test_set = prepare_data()\n",
    "    train_len = train_set['data'].shape[0]\n",
    "    test_len = test_set['data'].shape[0]\n",
    "    validation_len = validation_set['data'].shape[0]\n",
    "\n",
    "    accs = []\n",
    "    try:\n",
    "        for i in range(1,3):\n",
    "            tf.reset_default_graph()\n",
    "            is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "            # graph input\n",
    "            input = tf.placeholder(tf.float32, [None, N_INPUT,N_INPUT,N_CHANNEL], name='train_input')\n",
    "            # graph label\n",
    "            label = tf.placeholder(tf.float32, [None, N_CLASSES], name='train_label')\n",
    "\n",
    "            # predict\n",
    "            if i == 1:\n",
    "                file_writer.write('Test model of cnn_1 net....\\n')\n",
    "                file_writer.flush()\n",
    "                pred = cnn_1(input, is_training)\n",
    "            elif i == 2:\n",
    "                file_writer.write('Test model of cnn_2 net....\\n')\n",
    "                file_writer.flush()\n",
    "                pred = cnn_2(input, is_training)\n",
    "            else:\n",
    "                raise ValueError('Out of model')\n",
    "\n",
    "            # tu dong tim kiem learning_rate phu hop\n",
    "            global_step = tf.Variable(0, trainable=False)\n",
    "            learning_rate = tf.train.exponential_decay(\n",
    "                        1e-4,  # Base learning rate.\n",
    "                        global_step * BATCH_SIZE,  # Current index into the dataset.\n",
    "                        train_len,  # Decay step.\n",
    "                        0.95,  # Decay rate.\n",
    "                        staircase=True)\n",
    "\n",
    "            # dinh nghia cac ham mat mat\n",
    "            loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=label))\n",
    "            optimal = tf.train.AdamOptimizer(learning_rate)\n",
    "            train_op = optimal.minimize(loss_op, global_step=global_step)\n",
    "\n",
    "            # danh gia model\n",
    "            correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(label, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "            # tao summary cua cac monitor de quan sat cac bien\n",
    "            tf.summary.scalar('loss_op', loss_op)\n",
    "            tf.summary.scalar('learning_rate', learning_rate)\n",
    "            tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "            # gop cac summaries vao mot operation\n",
    "            merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "            # tao doi tuong log writer va ghi vao Tensorboard\n",
    "            writer = tf.summary.FileWriter('./checkpoint', graph=tf.get_default_graph())\n",
    "\n",
    "            # khoi tao cac variables\n",
    "            init = tf.global_variables_initializer()\n",
    "            # Add ops to save and restore all the variables.\n",
    "            saver = tf.train.Saver()\n",
    "            model_path = \"model_\" + str(i) + \"/cnn_model.ckpt\"\n",
    "\n",
    "            # training\n",
    "            start = time.time()\n",
    "            train()\n",
    "            elapsed = time.time() - start\n",
    "            file_writer.write('--- Training process is completed in %.3f (s) \\n' % (elapsed))\n",
    "            file_writer.flush()\n",
    "            \n",
    "            # test on validation set\n",
    "            start = time.time()\n",
    "            acc = evalute(validation_set, model_path)\n",
    "            elapsed = time.time() - start\n",
    "            file_writer.write('--- Evalution process on validate set is completed in %.3f (s) \\n' % (elapsed))\n",
    "            file_writer.write('-----------------------------------------------------------------')\n",
    "            file_writer.flush()\n",
    "            \n",
    "            accs.append(acc)\n",
    "\n",
    "\n",
    "        # test the best model on test set\n",
    "        best_model = accs.index(max(accs)) + 1\n",
    "        start = time.time()\n",
    "        model_path = \"model_\" + str(best_model) + \"/cnn_model.ckpt\"\n",
    "        acc = evalute(test_set, model_path)\n",
    "        elapsed = time.time() - start\n",
    "        file_writer.write('--- Evalution process on test set is completed in %.3f (s) \\n' % (elapsed))\n",
    "        file_writer.flush()\n",
    "        \n",
    "    except:\n",
    "        file_writer.write(traceback.format_exc())\n",
    "        file_writer.flush()\n",
    "        \n",
    "    file_writer.close()\n",
    "    print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
